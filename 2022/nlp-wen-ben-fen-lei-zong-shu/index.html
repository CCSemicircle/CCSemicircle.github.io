<!DOCTYPE HTML>
<html lang="zh-CN">

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4a943d1ca56da567bf16a9c4e2d1368e";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>
    
<link rel="stylesheet" href="/js/prism/prism.css">
<head>
    <meta charset="utf-8">
    <meta name="keywords" content="文本分类综述, NLP,软件工程">
    <meta name="description" content="文本分类的综述，主要基于《A Survey on Text Classification：From Shallow to Deep Learning》。">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id=G-7MRBCYRJPD"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-7MRBCYRJPD');
</script>


    <title>文本分类综述 | fdChen的掉发收集箱</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/favicon.png">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/css/loading.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/awesome/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/css/my.css">

    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="fdChen的掉发收集箱" type="application/atom+xml">
</head>



   <style>
    body{
       background-image: url(https://img.fdchen.host/wall.jpg);
       background-repeat:no-repeat;
       background-size: 100% 100%;
       background-attachment:fixed;
    }
</style>



<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    <div>
                        
                        <img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/medias/logo.png" class="logo-img" alt="LOGO">
                        
                        <span class="logo-span">fdChen的掉发收集箱</span>
                    </div>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">fdChen的掉发收集箱</div>
        <div class="logo-desc">
            
            fdChen的技术博客与生活日志
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/CCSemicircle" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/CCSemicircle" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://www.kaggle.com/static/images/text_classification.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">文本分类综述</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                <span class="chip bg-color">学习笔记</span>
                            </a>
                        
                            <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">
                                <span class="chip bg-color">文本分类</span>
                            </a>
                        
                            <a href="/tags/%E7%BB%BC%E8%BF%B0/">
                                <span class="chip bg-color">综述</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/NLP/" class="post-category">
                                NLP
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-10-12
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    55 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>文本主要基于《A Survey on Text Classification: From Shallow to Deep Learning》进行补充，原文章链接如下：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.00364">A Survey on Text Classification: From Shallow to Deep Learning</a></p>
</blockquote>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文通过回顾1961至2021最先进的方法填补了这一空白，重点关注从传统模型到深度学习的模型。我们根据所涉及的文本以及用于特征提取和分类的模型创建文本分类法。然后，我们详细讨论了这些类别中的每一个，涉及支持预测测试的技术发展和基准数据集。本调查还提供了不同技术之间的综合比较，以及确定各种评估指标的利弊。最后，我们总结了关键意义、未来研究方向和研究领域面临的挑战。</p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p><strong>文本分类——为文本指定预定义标签的过程</strong>——是许多自然语言处理（NLP）应用程序中的一项重要任务，如情感分析、主题标记、问答和对话行为分类。</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/%E5%85%B8%E5%9E%8B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="典型文本分类方法的流程图"></p>
<blockquote>
<p>传统方法的关键是提取基本特征，但深度学习方法自动提取特征。</p>
</blockquote>
<p>对于文本分类而言，最好使用机器学习方法来自动化文本分类过程，以产生更可靠和更少主观的结果。此外，这还可以通过定位所需信息来帮助提高信息检索效率，缓解信息过载的问题。</p>
<p><strong>第一个重要步骤是预处理模型的文本数据。</strong>传统模型通常需要通过人工方法获得良好的样本特征，然后用经典的机器学习算法进行分类。因此，特征提取在很大程度上限制了该方法的有效性。不同于传统模型，深度学习通过学习一组用于将特征直接映射到输出的非线性变换，将特征工程集成到模型拟合过程中。</p>
<p><strong>传统方法是指基于统计的模型</strong>，如Naive Bayes（NB）、K-Nearest Neighbor（KNN）和Support Vector Machine（SVM）。与早期的基于规则的方法相比，该方法在准确性和稳定性方面具有明显的优势。然而，这些方法仍然需要进行特征工程，这是耗时和昂贵的。此外，他们通常忽略文本数据中的自然顺序结构或上下文信息，这使得学习单词的语义信息变得困难。</p>
<p>与基于传统的方法相比，<strong>深度学习方法避免了人为设计规则和特征，并自动为文本挖掘提供语义上有意义的表示。</strong>因此，大多数文本分类研究工作都基于深度神经网络（DNN），这是一种计算复杂度高的数据驱动方法。</p>
<h2 id="1-1-主要内容"><a href="#1-1-主要内容" class="headerlink" title="1.1 主要内容"></a>1.1 主要内容</h2><ul>
<li>我们介绍了文本分类的过程和发展，并对主要模型进行了综合分析和研究。</li>
<li>我们介绍了现有的数据集，并通过比较给出了主要的评价指标，包括单标签和多标签文本分类任务。</li>
<li>我们总结了他们文章中给出的模型在基准数据集上的分类准确度得分，并通过讨论文本分类面临的主要挑战和本研究产生的关键影响来总结调查。</li>
</ul>
<h2 id="1-2-本文结构"><a href="#1-2-本文结构" class="headerlink" title="1.2 本文结构"></a>1.2 本文结构</h2><p>第2节总结了与文本分类相关的现有模型，包括传统模型和深度学习模型，包括汇总表。第3节介绍了主要数据集，包括单标签和多标签任务的汇总表和评估指标。然后，我们在第4节中给出了经典文本分类数据集中主要模型的定量结果。最后，我们在总结第6节中的文章之前，在第5节中总结了深度学习文本分类的主要挑战。</p>
<h1 id="2-文本分类方法"><a href="#2-文本分类方法" class="headerlink" title="2 文本分类方法"></a>2 文本分类方法</h1><p>对于传统模型，NB[8]是第一个用于文本分类任务的模型。然后，提出了通用分类模型，如KNN、SVM和Random Forest（RF），它们被称为分类器，广泛用于文本分类。最近，eXtreme Gradient Boosting（XGBoost）和Light Gradient Boosting Machine（LightGBM）有可能提供出色的性能。</p>
<p>对于深度学习模型，TextCNN在这些模型中引用最多，其中首次引入了卷积神经网络（CNN）模型来解决文本分类问题。虽然不是专门为处理文本分类任务而设计的，但考虑到其在许多文本分类数据集上的有效性，变压器的双向编码器表示法（BERT）在设计文本分类模型时得到了广泛应用。</p>
<h2 id="2-1-传统模型"><a href="#2-1-传统模型" class="headerlink" title="2.1 传统模型"></a>2.1 传统模型</h2><p>首先是对原始输入文本进行预处理，以训练传统模型，通常包括分词、数据清理和统计。</p>
<p>文本表示旨在以计算机更容易的形式表示预处理文本，并将信息损失降至最低，例如Bag-Of-Words（BOW）、N-gram、Term Frequency-Inverse Document（TF-IDF）[22]、word2vec[23]和Global Vecoter for word representation（Global Vectors）。</p>
<h3 id="2-1-1-文本表示"><a href="#2-1-1-文本表示" class="headerlink" title="2.1.1 文本表示"></a>2.1.1 文本表示</h3><h4 id="2-1-1-1-BOW"><a href="#2-1-1-1-BOW" class="headerlink" title="2.1.1.1 BOW"></a>2.1.1.1 BOW</h4><blockquote>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Elenstone/article/details/105134863">https://blog.csdn.net/Elenstone/article/details/105134863</a></p>
</blockquote>
<p>BOW（Bag of Words）词袋模型最初被用在文本分类中，将文档表示成特征矢量。<strong>它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是将每篇文档都看成一个袋子（因为里面装的都是词汇，所以称为词袋，Bag of words即因此而来），然后看这个袋子里装的都是些什么词汇，将其分类。</strong>如果文档中猪、马、牛、羊、山谷、土地、拖拉机这样的词汇多些，而银行、大厦、汽车、公园这样的词汇少些，我们就倾向于判断它是一篇描绘乡村的文档，而不是描述城镇的。</p>
<p>例如三个句子如下：</p>
<pre class="line-numbers language-none"><code class="language-none">句子1：小孩喜欢吃零食。
句子2：小孩喜欢玩游戏，不喜欢运动。
句子3 ：大人不喜欢吃零食，喜欢运动。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>首先根据语料中出现的句子分词，然后构建词袋（每一个出现的词都加进来）。计算机不认识字，只认识数字，那在计算机中怎么表示词袋模型呢？其实很简单，给每个词一个位置索引就可以了。小孩放在第一个位置，喜欢放在第二个位置，以此类推。</p>
<pre class="line-numbers language-none"><code class="language-none">&#123;“小孩”:1，“喜欢”:2，“吃”:3，“零食”:4，“玩”:5，“游戏”:6，“大人”:7，“不”:8，“运动”:9&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>其中key为词，value为词的索引，预料中共有9个单词， 那么每个文本我们就可以使用一个9维的向量来表示。<br>如果文本中含有的一个词出现了一次，就让那个词的位置置为1，词出现几次就置为几，那么上述文本可以表示为：</p>
<pre class="line-numbers language-none"><code class="language-none">句子1：[1,1,1,1,0,0,0,0,0]
句子2：[1,2,0,0,1,1,0,1,1]
句子3：[0,2,1,1,0,0,1,1,1]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>


<p>注意，该向量与原来文本中单词出现的顺序没有关系，仅仅是词典中每个单词在文本中出现的频率。</p>
<h4 id="2-1-1-2-N-gram"><a href="#2-1-1-2-N-gram" class="headerlink" title="2.1.1.2 N-gram"></a>2.1.1.2 N-gram</h4><p>N-gram考虑相邻单词的信息，并通过考虑相邻单词建立字典。它用于计算句子的概率模型。<strong>句子的概率表示为句子中每个单词的联合概率。</strong>句子的概率可以通过给定第(N−1）个单词，预测第N个单词。为了简化计算，<strong>N-gram模型采用马尔可夫假设，单词只与前面的单词有关。</strong>因此，<strong>N-gram模型执行大小为N的滑动窗口，通过计数和记录所有片段的出现频率，可以使用记录中相关片段的频率计算句子的概率。</strong></p>
<h4 id="2-1-1-3-TF-IDF"><a href="#2-1-1-3-TF-IDF" class="headerlink" title="2.1.1.3 TF-IDF"></a>2.1.1.3 TF-IDF</h4><p>TF-IDF使用单词频率除以逆文档频率来建模文本。<strong>TF是特定文章中某个单词的词频，IDF是包含该单词的文章占语料库中文章总数比例的倒数，TF-IDF是两者的乘法。</strong>TF-IDF评估单词对一组文件或语料库中一个文档的重要性。<strong>单词的重要性随其在文档中出现的次数成比例增加。然而，与它在整个语料库中的出现频率成反比。</strong>（成反比的目的是去除一些通用词的影响）</p>
<h4 id="2-1-1-4-word2vec"><a href="#2-1-1-4-word2vec" class="headerlink" title="2.1.1.4 word2vec"></a>2.1.1.4 word2vec</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/word2vec_structure.png" alt="word2vec算法结构"></p>
<p><strong>word2vec使用局部上下文信息来获取单词向量。</strong>单词向量是指为语料库中任何单词的单词向量，是固定长度实值向量。<strong>word2vec使用两种基本模型：CBOW和Skip-gram。前者是在已知当前单词上下文的前提下预测当前单词，后者是预测当前单词已知时的上下文。</strong></p>
<h4 id="2-1-1-5-Glove"><a href="#2-1-1-5-Glove" class="headerlink" title="2.1.1.5 Glove"></a>2.1.1.5 Glove</h4><p><strong>GloVe具有局部上下文和全局统计特征</strong>，它对单词-单词共现矩阵中的非零元素进行训练。它使单词向量能够包含尽可能多的语义和语法信息。<strong>单词向量的构造方法是：首先基于语料库构建单词的共现矩阵，然后基于共现矩阵和GloVe模型学习单词向量。</strong></p>
<p><strong>GloVe的实现分为以下三步</strong>（来自：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6b74f77c05e3">浅谈词嵌入（三）：GloVe详解</a>）：</p>
<ul>
<li><p>根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）$X$，<strong>矩阵中的每一个元素 $X_{ij}$ 代表单词 $i$ 和上下文单词$j$在特定大小的上下文窗口（context window）内共同出现的次数。</strong>一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离 <em>d</em>，提出了一个衰减函数（decreasing weighting）：*$decay&#x3D;1&#x2F;d$* 用于计算权重，也就是说<strong>距离越远的两个单词所占总计数（total count）的权重越小</strong>。</p>
</li>
<li><p>构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/%E8%AF%8D%E5%90%91%E9%87%8F%E4%B8%8E%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5%E7%9A%84%E5%85%B3%E7%B3%BB.png" alt="词向量与共现矩阵的关系"></p>
<p>其中，上式的$w_i$ 和 $w_j$是我们最终要求解的词向量；而 $b_i$和 $b_j$则是两个词向量的 bias 项。</p>
</li>
<li><p>有了公式（1）之后我们就可以构造损失函数了：</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/Glove_loss_function.png" alt="Glov损失函数"></p>
<p>这个loss function的基本形式就是最简单的mean square loss，只不过在此基础上加了一个权重函数$f(X_{ij})$， 那么这个函数起了什么作用，为什么要添加这个函数呢？我们知道在一个语料库中，肯定<strong>存在很多单词他们在一起出现的次数是很多的（frequent co-occurrences）</strong>，那么我们希望：</p>
<p>（1）这些单词的权重要大于那些很少在一起出现的单词（rare co-occurrences），所以这个函数要是非递减函数（non-decreasing）；</p>
<p>（2）但我们也不希望这个权重过大（overweighted），当到达一定程度之后应该不再增加；</p>
<p>（3）如果两个单词没有在一起出现，也就是$X_{ij}&#x3D; 0$，那么他们应该不参与到loss function的计算当中去，也就是$f(x)$要满足$f(0)&#x3D;0$；</p>
<p>满足以上两个条件的函数有很多，作者采用了如下形式的分段函数：</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/Glove_f(x).png" alt="Glove的f(x)函数"></p>
<p>$x$表示共现次数，而$x_max$表示最大共现次数。</p>
<p>这个函数图像如下所示：</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/Glove%E7%9A%84f(x)%E5%9B%BE%E5%83%8F.png" alt="Glove的f(x)图像"></p>
</li>
</ul>
<h3 id="2-1-2-表征分类器"><a href="#2-1-2-表征分类器" class="headerlink" title="2.1.2 表征分类器"></a>2.1.2 表征分类器</h3><h4 id="2-1-2-1-基于概率图模型的方法"><a href="#2-1-2-1-基于概率图模型的方法" class="headerlink" title="2.1.2.1 基于概率图模型的方法"></a>2.1.2.1 基于概率图模型的方法</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/NB_structure.png" alt="NB算法结构"></p>
<p>概率图形模型(Probabilistic Graphical Models，PGM)表示图中特征之间的条件依赖性，例如贝叶斯网络。它是概率论和图论的结合。</p>
<p>朴素贝叶斯（Naïve Bayes，NB）是应用贝叶斯定理的最简单、应用最广泛的模型。NB算法给予一个独立假设：当给定目标值时，文本之间的条件$T&#x3D;[T_1,T_2,…,T_n]$是独立的，如图。NB算法采用先验概率去计算后验概率，$P(y|T_1,T_2,…,T_n)&#x3D;\frac{p(y)\prod_{j&#x3D;1}^{n}{p(T_j|y)}}{\prod_{j&#x3D;1}^n{p(T_j)}}$。</p>
<p>伯努利NB（Bernoulli NB）、高斯NB（Gaussian NB）和多项式NB（Multinomial NB）是NB文本分类常用的三种方法。多项式NB在少数标记数据集上的性能略优于伯努利NB。在20新闻组（20 Newsgroups，20NG）和WebKB数据集上，高斯事件模型的贝叶斯NB分类器已被证明优于多项式事件模型的NB分类器。</p>
<h4 id="2-1-2-2-基于K近邻算法的方法"><a href="#2-1-2-2-基于K近邻算法的方法" class="headerlink" title="2.1.2.2 基于K近邻算法的方法"></a>2.1.2.2 基于K近邻算法的方法</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/KNN_structure.png" alt="KNN算法结构"></p>
<p>K近邻(K-Nearest Neighbors，KNN)算法的核心是通过在K个最近的样本上找到样本最多的类别来对一个未标记的样本进行分类。它是一个不需要建立模型的简单分类器，通过快速获取K个最近邻来降低复杂度。KNN算法的改进主要包括特征相似度、K值和指标优化。</p>
<p>但是由于KNN算法的时间&#x2F;空间复杂度与数据集大小成正相关，KNN算法在大规模数据集上会耗费大量的时间。</p>
<h4 id="2-1-2-3-基于支持向量机的方法"><a href="#2-1-2-3-基于支持向量机的方法" class="headerlink" title="2.1.2.3 基于支持向量机的方法"></a>2.1.2.3 基于支持向量机的方法</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/SVM_structre.png" alt="SVM算法结构"></p>
<p>基于支持向量机（Support Vector Machine，SVM）的方法将文本分类任务转化为多个二进制分类任务。在这种情况下，支持向量机在一维输入空间或特征空间中构造一个最优超平面，使超平面与两类训练集之间的距离最大化，从而获得最佳的泛化能力。目的是使类别边界沿垂直于超平面方向的距离最大。同样，这将导致分类错误率最低。构造最优超平面可以转化为二次规划问题，得到全局最优解。选择合适的核函数和特征选择对于保证支持向量机能够处理非线性问题，成为一个鲁棒的非线性分类器至关重要。在监督学习算法支持向量机的基础上，采用主动学习和自适应学习方法进行文本分类，减少标注工作量。</p>
<h4 id="2-1-2-4-基于决策树的方法"><a href="#2-1-2-4-基于决策树的方法" class="headerlink" title="2.1.2.4 基于决策树的方法"></a>2.1.2.4 基于决策树的方法</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/DT_structure.png" alt="DT算法结构"></p>
<p>决策树（Decision Trees，DT）是一种有监督的树结构学习方法，反映了分治（divide-and-conquer）的思想，是递归构造的。决策树通常可以分为两个不同的阶段：树的构建和树的修剪。它从根节点开始，测试数据样本（由实例集组成，具有几个属性），并根据不同的结果将数据集划分为不同的子集。数据集的子集构成子节点，决策树中的每个叶节点代表一个类别。构建决策树是为了确定类与属性之间的相关性，进一步利用决策树预测未知即将出现类型的记录类别。决策树算法生成的分类规则简单明了，剪枝策略也有助于减少噪声的影响。然而，它的局限性主要来自于处理爆炸式增长的数据量的效率低下。</p>
<p>迭代二分器3（Iterative Dichotomiser 3，ID3）算法在每个节点的选择中使用信息增益作为属性选择准则——它用于选择每个分支节点的属性，然后选择具有最大信息增益值的属性成为当前节点的判别属性。C4.5基于ID3学习获取从属性到类的映射，有效地对新类别所未知的实体进行分类。</p>
<h4 id="2-1-2-5-基于集成算法的方法"><a href="#2-1-2-5-基于集成算法的方法" class="headerlink" title="2.1.2.5 基于集成算法的方法"></a>2.1.2.5 基于集成算法的方法</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/RF_structure.png" alt="RF算法结构"></p>
<p>集成算法旨在聚合多个算法的结果，以获得更好的性能和解释。传统的集成算法有自举聚合（bootstrap），如RF，增强（boosting），如自适应增强（AdaBoost）和XGBoost和堆叠。</p>
<p>自举聚合方法训练多个没有强依赖关系的分类器，然后聚合它们的结果。例如，RF由多个树分类器组成，其中所有树都依赖于独立采样的随机向量的值。值得注意的是，RF中的每棵树共享相同的分布。RF的泛化误差依赖于每棵树的强度和树之间的关系，并随着森林中树数的增加而收敛到一个极限。</p>
<p>在基于boosting的算法中，对所有标记数据进行相同权值的训练，初始得到较弱的分类器。然后根据分类器之前的结果调整数据的权重。训练过程将继续重复这些步骤，直到达到终止条件。</p>
<p>与bootstrap和boosting算法不同，基于堆叠的算法将数据分解为n部分，并使用n个分类器以级联方式计算输入数据——上游分类器的结果将输入到下游分类器。一旦目标是预定义的迭代次数，训练将终止。</p>
<h4 id="2-1-2-6-总结"><a href="#2-1-2-6-总结" class="headerlink" title="2.1.2.6 总结"></a>2.1.2.6 总结</h4><p>NB的参数更小，对缺失数据不敏感，算法简单。然而，它假设特征是相互独立的。当特征数量较大，或特征之间的相关性显著时，NB的性能下降。支持向量机可以解决高维和非线性问题。它具有较高的泛化能力，但对缺失数据比较敏感。KNN主要依靠周围的有限相邻样本，而不是判别类域来确定类别。因此，对于类域交叉或重叠较多的数据集划分，该方法比其他方法更适合。DT很容易理解和解释。给定一个观察到的模型，根据生成的决策树很容易推导出相应的逻辑表达式。</p>
<p>传统的方法是一种机器学习。它从数据中学习，这些预定义的特征对预测性能很重要。然而，特性工程是一项艰巨的工作。在训练分类器之前，我们需要收集知识或经验，从原始文本中提取特征。传统方法根据从原始文本中提取的各种文本特征训练初始分类器。对于小数据集，传统模型在计算复杂度的限制下，通常表现出比深度学习模型更好的性能。因此，一些研究者针对数据较少的特定领域研究了传统模型的设计。</p>
<h2 id="2-2-深度学习模型"><a href="#2-2-深度学习模型" class="headerlink" title="2.2 深度学习模型"></a>2.2 深度学习模型</h2><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF.png" alt="不同模型的基本信息"></p>
<h3 id="2-2-1-基于ReNN的方法"><a href="#2-2-1-基于ReNN的方法" class="headerlink" title="2.2.1 基于ReNN的方法"></a>2.2.1 基于ReNN的方法</h3><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/ReNN_structure.png" alt="ReNN算法结构"></p>
<p>传统模型在为每个任务设计特性上花费了大量的时间。由此，在深度学习的情况下，”词向量”的含义是不同的：每个输入词都与一个固定长度的向量相关联，其值要么是随机抽取的，要么是从之前的传统过程中推导出来的，从而形成一个矩阵L，称为词嵌入矩阵（word embedding matrix），以将词典词汇映射到潜较小语义空间中，一般为50到300维。</p>
<p>递归神经网络（Recursive Neural Network, ReNN）可以自动递归学习文本语义和语法树结构，无需特征设计，如图所示，我们给出了一个基于ReNN模型的例子。<strong>首先，将输入文本的每个单词作为模型结构的叶节点。然后使用权重矩阵将所有节点组合成父节点，权重矩阵在整个模型中共享，每个父节点与所有叶节点具有相同的维数。最后，将所有节点递归聚合到一个父节点中，以表示预测标签的输入文本。</strong></p>
<h3 id="2-2-2-基于MLP的方法"><a href="#2-2-2-基于MLP的方法" class="headerlink" title="2.2.2 基于MLP的方法"></a>2.2.2 基于MLP的方法</h3><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/MLP-structure.png" alt="MLP算法结构"></p>
<p>多层感知器（MultiLayer Perceptron, MLP），有时通俗地称为“香草”神经网络，是一种用于自动捕获特征的简单神经网络结构。如图所示，我们给出了一个三层MLP模型。<strong>它包含一个输入层、一个在所有节点中具有激活函数的隐藏层和一个输出层。每个节点都以一定的权重$w_i$连接。</strong>它将每个输入文本视为一个单词包，与传统模型相比，在许多文本分类基准测试中都取得了较高的性能。</p>
<h3 id="2-2-3-基于RNN的方法"><a href="#2-2-3-基于RNN的方法" class="headerlink" title="2.2.3 基于RNN的方法"></a>2.2.3 基于RNN的方法</h3><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/RNN_structure.png" alt="RNN算法结构"></p>
<p>循环神经网络（Recurrent Neural Network，RNN）被广泛用于通过循环计算获取远程依赖性。RNN语言模型学习历史信息，考虑适合文本分类任务的所有词之间的位置信息。我们展示了一个简单样本的文本分类RNN模型，如图所示。<strong>首先，利用词嵌入技术将每个输入词用一个特定的向量表示。然后，将嵌入词向量逐个送入RNN细胞。RNN细胞的输出与输入向量的维数相同，并被馈送到下一隐藏层。RNN在模型的不同部分共享参数，并对每个输入单词具有相同的权重。最后，通过隐层的最后一个输出来预测输入文本的标签。</strong></p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/RNN_and_LSTM.png" alt="RNN与LSTM对比"></p>
<p>在RNN的反向传播过程中，权重通过梯度调整，通过导数的连续乘法计算。如果导数非常小，则通过连续乘法可能引起梯度消失问题。<strong>长短期记忆（Long Short-Term Memory，LSTM）是对RNN的改进，有效地缓解了梯度消失问题。它由一个记忆任意时间间隔值的单元和三个控制信息流的门结构组成。门结构包括输入门、遗忘门和输出门。LSTM分类方法可以更好地捕获上下文特征词之间的联系，并利用遗忘门结构过滤无用信息，有利于提高分类器的总体捕获能力。</strong></p>
<h3 id="2-2-4-基于CNN的方法"><a href="#2-2-4-基于CNN的方法" class="headerlink" title="2.2.4 基于CNN的方法"></a>2.2.4 基于CNN的方法</h3><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/CNN_structure.png" alt="CNN算法结构"></p>
<p>CNN可以同时对序列的多个块应用由不同内核定义的卷积。对于文本分类，需要将文本表示为类似于图像表示的向量，并且可以从多个角度对文本特征进行过滤，如图所示。<strong>首先，将输入文本的词向量拼接成一个矩阵。然后将矩阵输入卷积层，卷积层包含几个不同维度的滤波器。最后，卷积层的结果经过池化层，并将池化结果连接起来，得到文本的最终向量表示。类别由最后一个向量预测。</strong></p>
<p>为了尝试使用CNN进行文本分类任务，Kim引入了一个卷积神经网络的无偏模型，称为TextCNN。它可以通过一层卷积更好地确定最大池化层中的判别短语，并保持词向量固定以学习除词向量外的超参数。</p>
<p>根据文本的最小嵌入单元，将嵌入方法分为字符级、词级和句子级三种。字符级嵌入可以解决词汇溢出（Out-Of-Vocabulary，OOV）词。单词级嵌入学习单词的语法和语义。此外，句子级嵌入可以捕捉句子之间的关系。基于这些动机，Nguyen等人提出了一种基于字典的深度学习方法，通过构建语义规则和用于字符级嵌入的深度CNN来增加单词级嵌入的信息。</p>
<p><strong>基于RNN的模型捕获序列信息来学习输入词之间的依赖关系，基于CNN的模型从卷积核中提取相似特征。</strong></p>
<h3 id="2-2-5-基于Attention的方法"><a href="#2-2-5-基于Attention的方法" class="headerlink" title="2.2.5 基于Attention的方法"></a>2.2.5 基于Attention的方法</h3><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/HAN_structure.png" alt="HAN算法结构"></p>
<p>Bahdanau等人首先提出了一种可用于机器翻译的注意机制。基于此，Yang等人引入了分层注意网络（Hierarchical Attention Network，HAN），通过使用文本的信息成分来获得更好的可视化效果，如图所示。<strong>HAN包括两个编码器和两个层次的注意层。注意机制允许模型对特定的输入进行不同的注意。它首先将基本词聚合成句子向量，然后将关键句向量聚合成文本向量。它可以了解每个单词和句子对分类判断的贡献有多大，这有利于通过两个层次的注意进行应用和分析。</strong>注意机制可以提高文本分类的性能和可解释性，因此得到广泛应用。</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/self-attention.png" alt="self-attention结构"></p>
<p><strong>自注意机制通过在句子之间构建K、Q和V矩阵来捕获单词在句子中的权重分布，这些矩阵可以捕获对文本分类的长期依赖性</strong>，如图所示。<strong>每个输入词向量$a_i$可以表示为三个n维向量，分别是$q_i,k_i,v_i$。经过自注意层后，输出向量$b_i$可以表示为 $\sum_j{softmax(a_{ij})v_j}$，其中$a_{ij}&#x3D;q_i\cdot k_j&#x2F;\sqrt{n}$。</strong>注意，所有的输出向量都可以并行计算。</p>
<h3 id="2-2-6-基于预训练的方法"><a href="#2-2-6-基于预训练的方法" class="headerlink" title="2.2.6 基于预训练的方法"></a>2.2.6 基于预训练的方法</h3><h4 id="2-2-6-1-ELMo、GPT、BERT"><a href="#2-2-6-1-ELMo、GPT、BERT" class="headerlink" title="2.2.6.1 ELMo、GPT、BERT"></a>2.2.6.1 ELMo、GPT、BERT</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/ELMo_GPT_BERT.png" alt="ELMo、GPT、BETR结构对比"></p>
<p><strong>预训练语言模型有效地学习全局语义表示，并显著促进NLP任务，包括文本分类。它通常使用无监督方法自动挖掘语义知识，然后构建预训练目标，使机器能够学习理解语义。</strong></p>
<p>如图所示，我们给出了Embedding from Language model（ELMo）、OpenAI GPT和BERT之间的模型架构差异。</p>
<p><strong>ELMo</strong>是一个通过深度挖掘上下文信息的单词表示模型，它很容易集成到模型中，它可以模拟单词的复杂特征，并学习不同语言上下文的不同表示。采用双向LSTM，根据上下文词学习各词嵌入。</p>
<p><strong>GPT</strong>采用有监督的微调和无监督的预训练来学习对许多NLP任务具有有限适应能力的一般表示。此外，目标数据集的域不需要与未标记数据集的域相似。GPT算法的训练过程通常包括两个阶段。首先，神经网络模型的初始参数由建模目标在未标记的数据集上学习。然后，我们可以使用相应的监督目标来适应目标任务的这些参数。</p>
<p>为了通过对每层的左右上下文进行联合调节，以预训练来自未标记文本的深度双向表示，谷歌提出的<strong>BERT</strong>模型显著提高了NLP任务的性能，包括文本分类。BERT采用双向编码器，通过联合调整所有层的上下文来预训练深度的双向表示。在预测哪些单词被屏蔽时，它可以利用上下文信息。通过添加额外的输出层来为多个NLP任务（如SA、QA和机器翻译）构建模型，可以对它进行微调。</p>
<p><strong>将这三种模型进行对比，ELMo是使用LSTM的基于特征的方法，BERT和OpenAI GPT是使用Transformer的微调方法。ELMo和BERT是双向训练模型，OpenAI GPT是从左向右训练。因此，结合ELMo和OpenAI GPT的优点，BERT得到了更好的结果。</strong></p>
<h4 id="2-2-6-2-RoBETRa、XLNet、ALBERT"><a href="#2-2-6-2-RoBETRa、XLNet、ALBERT" class="headerlink" title="2.2.6.2 RoBETRa、XLNet、ALBERT"></a>2.2.6.2 RoBETRa、XLNet、ALBERT</h4><p>基于Transformer的模型可以并行计算，而无需考虑适合于大规模数据集的顺序信息，这使得它在NLP任务中很受欢迎。因此，其他一些工作被用于文本分类任务，并获得了良好的性能。<strong>RoBERTa是BERT的改进版本，采用动态屏蔽方法生成屏蔽样式，并将一个序列输入模型。它使用更多的数据进行更长时间的预训练，并估计各种基本超参数的影响和训练数据的大小。</strong>具体来说：1）训练时间更长（共近20万次训练，学习近16亿次训练数据），批处理规模（8K）更大，训练数据更多（30G中文训练，含3亿句，100亿字）；2）去除下一句预测（Next Sentence Prediction，NSP）任务；3）采用更扩展的训练序列；4）动态调整屏蔽机制，使用全字掩码。</p>
<p><strong>XLNet是一种广义自回归预训练方法。与BERT不同的是，第一阶段不使用带掩码的去噪自编码器，而是使用自回归LM。它在一个句子内 所有单词顺序随机排列中，最大化期望似然来学习双向上下文。此外，它可以通过自回归公式克服BERT的缺点，并将Transformer-XL的想法整合到预训练中。</strong>XLNet的详解如下（来自：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/110204573">XLNet详解</a>）：</p>
<p><strong>AR语言模型（Auto-Regressive Language Modeling）：</strong></p>
<ul>
<li><p><strong>依据前面（或后面）出现的tokens来预测当前时刻的token</strong>，代表有 ELMO， GPT等。</p>
</li>
<li><p><strong>缺点：</strong>它只能利用单向语义而<strong>不能同时利用上下文信息</strong>。 ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。</p>
</li>
<li><p><strong>优点：</strong> 对生成模型友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。</p>
</li>
</ul>
<p><strong>AE语言模型（Auto-Encoding Language Modeling）：</strong></p>
<ul>
<li><p><strong>通过上下文信息来预测被mask的token，代表有 BERT , Word2Vec(CBOW) 。</strong></p>
</li>
<li><p><strong>缺点：</strong> 由于训练中采用了 [MASK] 标记，导致<strong>预训练与微调阶段不一致的问题</strong>。BERT独立性假设问题，即没有对<strong>被遮掩（Mask）的 token 之间的关系</strong>进行学习。 此外对于生成式问题， AE 模型也显得捉襟见肘。</p>
</li>
<li><p><strong>优点：</strong> 能够很好的编码上下文语义信息（即考虑句子的双向信息）， 在自然语言理解相关的下游任务上表现突出。</p>
</li>
</ul>
<p><strong>XLNet的方法（结合AR和AE模型的优点）：</strong></p>
<p>XLNet提出<strong>Permutation Language Model（PLM），具体实现方式是，通过随机取一句话的一种排列，然后将末尾一定量的词给“遮掩”（和 BERT 里的直接替换 “[MASK]” 有些不同）掉，最后用 AR 的方式来按照这种排列依次预测被“遮掩”掉的词。</strong></p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/PermutationLM_idea.png" alt="PLM的思想"></p>
<p>我们可以发现通过随机取排列（Permutation）中的一种，就能非常巧妙地通过 <strong>AR 的单向方式来习得双向信息</strong>了。论文中 Permutation 具体的实现方式是通过直接对 Transformer 的 <strong>Attention Mask</strong> 进行操作。</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/PermutationLM_AttentionMask.png" alt="PLM的Attention Mask"></p>
<p>比如说序号依次为 1234 的句子，先随机取一种排列3241。于是根据这个排列我们就做出类似上图的 Attention Mask。先看第1行，因为在<strong>新的排列方式中 1 在最后一个，根据从左到右 AR 方式，1 就能看到 234 全部，于是第一行的 234 位置是红色的（没有遮盖掉，会用到），以此类推。第2行，因为 2 在新排列是第二个，只能看到 3，于是 3 位置是红色。第 3 行，因为 3 在第一个，看不到其他位置，所以全部遮盖掉，以此类推。</strong></p>
<p><strong>XLNet对于PLM理念的实现（Two-Stream Self-Attention）：</strong></p>
<p>为了实现 Permutation 加上 AR 预测过程，首先我们会发现，打乱顺序后位置信息非常重要，同时对每个位置来说，需要预测的是内容信息（对应位置的词），于是输入就不能包含内容信息，不然模型学不到东西，只需要直接从输入复制到输出就好了。</p>
<p>于是这里就造成了位置信息与内容信息的割裂，因此在 BERT 这样的<strong>位置信息加内容信息输入 Self-Attention (自注意力) 的流（Stream）</strong>之外，作者还增加了另一个<strong>只有位置信息作为 Self-Attention 中 query 输入的流</strong>。文中将前者称为 <strong>Content Stream</strong>，而后者称为 <strong>Query Stream</strong>。</p>
<p>这样就能利用 Query Stream 在对需要预测位置进行预测的同时，又不会泄露当前位置的内容信息（<strong>因为仅采用了当前位置信息与其他位置的内容信息</strong>）。具体操作就是用两组隐状态（hidden states） $g$ 和 $h$ 。其中 $g$ 只有位置信息，作为 Self-Attention 里的 Q。 $h$ 包含内容信息，则作为 K 和 V。具体表示如下图所示：</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/PermutationLM_OneLayerAttention.png" alt="PLM的一层双流self-attention"></p>
<p>假如，模型只有一层的话，其实这样只有 Query Stream 就已经够了。但如果将层数加上去的话，为了取得更高层的 h，于是就需要 Content Stream 了。h 同时作为 Q K V。如下图所示：</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/PermutationLM_MultiLayerAttention.png" alt="PLM的多层双流self-attention"></p>
<p>于是组合起来就是这样：</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/XLNet_self-attention.png" alt="XLNet的self-attention"></p>
<p>上图中我们需要理解两点：</p>
<ul>
<li>第一点，最下面一层蓝色的 Content Stream 的输入是$e(x_i) $，这个很好懂就是 $x$ 对应的词向量 (Embedding)，不同词对应不同向量，但看旁边绿色的 Query Stream，就会觉得很奇怪，为什么都是一样的 $w$ ？这个和Relative Positional Encoding 有关。</li>
<li>第二点，Query stream attention图中为了便于说明，只将当前位置之外的 h 作为 K 和 V，但实际上实现中应该是所有时序上的 h 都作为 K 和 V，最后再交给上图中的 Query stream 的 Attention Mask 来完成位置的遮盖。</li>
</ul>
<p><strong>Transformer-XL：</strong></p>
<ul>
<li><strong>RNN：</strong><ul>
<li>优点：支持可变长，支持记忆，有序列顺序关系；</li>
<li>缺点：gradient vanish，耗时无法并行。</li>
</ul>
</li>
<li><strong>Transformer：</strong><ul>
<li>优点：并行，考虑到sequence的long term dependency信息（相对于RNN），可解释性。</li>
<li>缺点：句子与句子之间的关系，batch size也不能很大，空间占用大（因为每个encoder的score matrix（sequenceLen*sequecenLen是 $N^2$ 的空间复杂度）</li>
</ul>
</li>
</ul>
<p>Transformer编码固定长度的上下文，即将一个长的文本序列截断为几百个字符的固定长度片段(segment)，然后分别编码每个片段，片段之间没有任何的信息交互。比如BERT，序列长度的极限一般在512。因此Transformer-XL提出的动机总结如下：</p>
<ul>
<li>Transformer无法建模超过固定长度的依赖关系，对长文本编码效果差。</li>
<li>Transformer把要处理的文本分割成等长的片段，通常不考虑句子（语义）边界，导致**上下文碎片化(context fragmentation)**。通俗来讲，一个完整的句子在分割后，一半在前面的片段，一半在后面的片段。</li>
</ul>
<p>《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》围绕如何建模长距离依赖，提出Transformer-XL（XL是extra long的意思）：</p>
<ul>
<li>提出**片段级循环机制(segment-level recurrence mechanism)<strong>，引入一个</strong>记忆(memory)**模块（类似于cache或cell），循环用来建模片段之间的联系。这使得长距离依赖的建模成为可能；也使得片段之间产生交互，解决上下文碎片化问题。</li>
<li>提出**相对位置编码机制(relative position embedding scheme)**，代替绝对位置编码。在memory的循环计算过程中，避免时序混淆，位置编码可重用。</li>
</ul>
<p><strong>Transformer-XL总结</strong>：片段级循环机制为了解决编码长距离依赖和上下文碎片化，相对位置编码机制为了实现片段级循环机制而提出，解决可能出现的时序混淆问题。也可以简单的理解Transformer-XL&#x3D;Transformer + RNN。</p>
<p>BERT模型有很多参数。为了减少参数，<strong>ALBERT</strong>采用双参数简化方案。它减少了碎片向量的长度，并与所有编码器共享参数。它还将下句匹配任务替换为下句顺序任务，并持续阻塞碎片。在海量中文语料库上对ALBERT模型进行预训练，得到的参数较少，性能较好。</p>
<h4 id="2-2-6-3-BART、SpanBERT、ERNIE"><a href="#2-2-6-3-BART、SpanBERT、ERNIE" class="headerlink" title="2.2.6.3 BART、SpanBERT、ERNIE"></a>2.2.6.3 BART、SpanBERT、ERNIE</h4><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/BART_structure.png" alt="BART算法结构"></p>
<p><strong>BART（Bidirectional and Auto-Regressive Transformers）</strong> 是基于Seq2Seq模型的去噪自动编码器，如图所示。BART的预训练包括两个步骤。<strong>首先，它使用噪声函数来破坏文本。其次，利用Seq2Seq模型对原文进行重构。</strong>在各种噪声方法中，通过对原句的顺序进行随机洗牌，然后使用第一个新文本填充方法来获得最佳性能。新文本填充方法是用单个掩码令牌替换文本片段。它只使用特定的屏蔽令牌来指示某个令牌被屏蔽。</p>
<p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/SpanBERT_structure.png" alt="SpanBERT算法结构"></p>
<p><strong>SpanBERT</strong> 是专门为更好地表示和预测文本跨度而设计的。它从三个方面优化了BERT，在QA等多个任务中都取得了很好的效果。具体优化体现在三个方面。<strong>首先，提出了片段掩码方案，对连续段落文本进行随机掩码。其次，使用片段边界目标（Span Boundary Objective，SBO）来预测被mask的片段，具体而言，SBO使用的是被masked 的span 的左右边界的字（未masked的）以及被mask的字的position，来预测当前被mask的片段。第三，取消了NSP的预训练任务。</strong></p>
<p><strong>ERNIE</strong> 基于知识增强的方法。它通过对海量数据集中的实体概念等先验语义知识建模，学习现实世界中的语义关系。具体来说，<strong>ERNIE使模型能够通过屏蔽词汇和实体等语义单位来学习完整概念的语义表示。它主要由一个Transformer编码器和任务嵌入组成。在Transformer编码器中，每个标记的上下文信息由自注意机制捕获，并生成用于嵌入的上下文表示。任务嵌入用于不同特征的任务。</strong></p>
<h3 id="2-2-7-基于GNN的方法"><a href="#2-2-7-基于GNN的方法" class="headerlink" title="2.2.7 基于GNN的方法"></a>2.2.7 基于GNN的方法</h3><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/GNN_structure.png" alt="GNN算法结构"></p>
<p>随着图神经网络(Graph Neural Networks，GNN)的日益重视，基于GNN的模型通过对句子的句法结构进行语义角色标注任务、关系分类任务和机器翻译任务进行编码，获得了优异的性能。它将文本分类转化为图节点分类任务。我们展示了一个具有四个输入文本的文本分类GCN模型，如图所示。</p>
<p><strong>首先，这四个输入文本$T&#x3D;[T_1, T_2, T_3, T_4]$和文本中的单词$X&#x3D; [x_1,x_2,x_3,x_4,x_5,x_6]$组成节点，并且构建图结构。每个词的词边权重通常表示它们在语料库中的共现频率。然后，通过隐层对单词和文本进行表示。最后，通过图可以预测所有输入文本的标签。</strong></p>
<h3 id="2-2-8-其他方法"><a href="#2-2-8-其他方法" class="headerlink" title="2.2.8 其他方法"></a>2.2.8 其他方法</h3><h4 id="2-2-8-1-Siamese-Neural-Network"><a href="#2-2-8-1-Siamese-Neural-Network" class="headerlink" title="2.2.8.1 Siamese Neural Network"></a>2.2.8.1 Siamese Neural Network</h4><p>连体神经网络（Siamese Neural Network）也被称为孪生神经网络（Twin NN）。它利用相等的权重，同时使用两个不同的输入向量来计算可比较的输出向量。</p>
<h4 id="2-2-8-2-Virtual-Adversarial-Training（VAT）"><a href="#2-2-8-2-Virtual-Adversarial-Training（VAT）" class="headerlink" title="2.2.8.2 Virtual Adversarial Training（VAT）"></a>2.2.8.2 Virtual Adversarial Training（VAT）</h4><p>深度学习方法需要大量额外的超参数，增加了计算复杂度。VAT基于局部分布光滑性的正则化可以用于半监督任务，只需要一些超参数，可以直接解释为鲁棒优化。</p>
<h4 id="2-2-8-3-Reinforcement-Learning（RT）"><a href="#2-2-8-3-Reinforcement-Learning（RT）" class="headerlink" title="2.2.8.3 Reinforcement Learning（RT）"></a>2.2.8.3 Reinforcement Learning（RT）</h4><p>RL通过最大化累积奖励来学习在特定环境中的最佳行动。</p>
<h4 id="2-2-8-4-Memory-Networks"><a href="#2-2-8-4-Memory-Networks" class="headerlink" title="2.2.8.4 Memory Networks"></a>2.2.8.4 Memory Networks</h4><p>记忆网络（Memory Networks）学会将推理组件和长期记忆组件结合起来。</p>
<h4 id="2-2-8-5-QA-Style-for-Sentiment-Classification-Task"><a href="#2-2-8-5-QA-Style-for-Sentiment-Classification-Task" class="headerlink" title="2.2.8.5 QA Style for Sentiment Classification Task"></a>2.2.8.5 QA Style for Sentiment Classification Task</h4><p>将情感分类任务视为QA任务是一个有趣的尝试。</p>
<h4 id="2-2-8-6-External-Commonsense-Knowledge"><a href="#2-2-8-6-External-Commonsense-Knowledge" class="headerlink" title="2.2.8.6 External Commonsense Knowledge"></a>2.2.8.6 External Commonsense Knowledge</h4><h4 id="2-2-8-7-Quantum-Language-Model"><a href="#2-2-8-7-Quantum-Language-Model" class="headerlink" title="2.2.8.7 Quantum Language Model"></a>2.2.8.7 Quantum Language Model</h4><p>在量子语言模（Quantum Language Model）中，单词和单词之间的依赖关系是通过基本量子事件来表示的。</p>
<h3 id="2-2-9-总结"><a href="#2-2-9-总结" class="headerlink" title="2.2.9 总结"></a>2.2.9 总结</h3><p>RNN按顺序计算，不能并行计算。RNN的缺点使得它在当前模型参数越来越多、越来越深入的趋势下成为主流更具挑战性。</p>
<p>CNN通过卷积核从文本向量中提取特征。卷积核捕获的特征的数量与它的大小有关。理论上，CNN的深度足以捕捉到远距离的特征。由于深层网络参数优化方法不足，加之池化层导致位置信息丢失，深层网络并没有带来明显的改善。与RNN相比，CNN具有并行计算能力，可以有效地保留改进版CNN的位置信息。但是，它的远距离特征捕获能力较弱。</p>
<p>GNN为文本构建一个图形。当设计一个有效的图结构时，学习表示可以更好地捕获结构信息。</p>
<p>Transformer将输入文本视为一个完全连通的图，在边缘上有注意评分权重。它具有并行计算的能力，能够通过自我注意高效地提取不同单词之间的特征，解决短时记忆问题。然而，Transformer中的注意机制计算量很大，特别是在处理长序列时。最近提出了一些改进的Transformer计算复杂度模型。</p>
<p>总的来说，Transformer是文本分类的更好选择。</p>
<p>深度学习由复杂程度较高的神经网络中的多个隐藏层组成，可以在非结构化数据上进行训练。深度学习可以学习语言特征，在词汇和向量的基础上掌握更高层次、更抽象的语言特征。深度学习架构可以直接从输入中学习特征表示，无需过多的人工干预和先验知识。然而，深度学习技术是一种数据驱动的方法，需要大量的数据才能实现高性能。虽然基于自注意机制的模型可以为DNN带来一些词之间的可解释性，但与传统模型相比，它不足以解释DNN为何以及如何有效工作。</p>
<h1 id="3-数据集与评价指标"><a href="#3-数据集与评价指标" class="headerlink" title="3 数据集与评价指标"></a>3 数据集与评价指标</h1><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/TextClassificationDataset.png" alt="文本分类数据集"></p>
<blockquote>
<p>PS：C代表类别数，L代表句子长度，N代表数据集大小</p>
</blockquote>
<h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><h3 id="3-1-1-Sentiment-Analysis-SA"><a href="#3-1-1-Sentiment-Analysis-SA" class="headerlink" title="3.1.1 Sentiment Analysis (SA)"></a>3.1.1 Sentiment Analysis (SA)</h3><p>SA是在情感色彩中对主观文本进行分析和推理的过程。从分析文本客观内容的传统文本分类中，获取到是否支持文本中的某个特定观点的信息是至关重要的。SA可以是二进制的也可以是多类的。二元SA是将文本分为两类，包括正面和负面。多类SA将文本分类为多级或细粒度标签。SA数据集包括电影评论(Movie Review，MR)、斯坦福情感树库(Stanford Sentiment Treebank，SST)、多视角问答(Multi-Perspective Question Answering，MPQA)、IMDB、Yelp、Amazon评论(Amazon Reviews ，AM)、NLP&amp;CC 2013、Subj、CR、SS-Twitter、SS-Youtube、SE1604等。</p>
<ul>
<li><strong>MR</strong>：MR是一个电影评论数据集，每一个都对应一个句子。该语料库共有5331个阳性数据和5331个阴性数据。随机分成10折交叉验证常用于MR的检验。</li>
<li><strong>SST</strong>：SST是MR的扩展，它有两种类别。SST-1带有五个类的细粒度标签，它分别有8544个训练文本和2210个测试文本。SST-2有9,613个带有二进制标签的文本，被划分为6920个训练文本、872个开发文本和1821个测试文本。</li>
<li><strong>MPQA</strong>：MPQA是一个观点数据集。它有两个类标签和一个MPQA数据集的意见极性检测子任务。MPQA包括10,606个句子，摘自各种新闻来源的新闻文章。应当指出的是，它包含3 311个积极文本和7 293个消极文本，每一文本都没有标明。</li>
<li><strong>IMDB reviews</strong>：IMDB评论是为每一类相同数量的电影评论的二元情感分类而开发的。它可以平均分为训练组和测试组，每组有25000条评论。</li>
<li><strong>Yelp reviews</strong>：Yelp的评论总结自2013年、2014年和2015年的Yelp数据集挑战。这个数据集有两个类别。其中的Yelp-2用于消极和积极情绪分类任务，包括56万篇训练文本和3.8万篇测试文本。Yelp-5用于检测细粒度情感标签，包含所有类中的650,000个训练文本和50,000个测试文本。</li>
<li><strong>AM</strong>：AM是一个通过收集亚马逊网站产品评论而形成的流行语料库。这个数据集有两个类别。亚马逊-2有两个级别，包括360万套训练集和40万套测试集。Amazon-5有五个类，包括300万条和65万条用于培训和测试的评论。</li>
</ul>
<h3 id="3-1-2-News-Classification-NC"><a href="#3-1-2-News-Classification-NC" class="headerlink" title="3.1.2 News Classification (NC)"></a>3.1.2 News Classification (NC)</h3><p>新闻内容是对人们产生重要影响的最重要的信息来源之一。NC系统方便用户实时获取重要信息。新闻分类应用主要包括：识别新闻主题和根据用户兴趣推荐相关新闻。新闻分类数据集包括20个新闻组(20 Newsgroups，20NG)、AG news (AG)、R8、R52、搜狗news(Sogou News，Sogou)等。</p>
<ul>
<li><strong>20NG</strong>：20NG是一个新闻组文本数据集。它有20个类别，每个类别的数量相同，包括18,846个文本。</li>
<li><strong>AG</strong>：AG News是一个搜索来自学术界的新闻的搜索引擎，选择了四个最大的类别。它使用新闻的标题和描述填充每个新闻字段。AG包含12万份训练文本和7 600份测试文本。</li>
<li><strong>R8 and R52</strong>：R8和R52是是Reuters的两个子集。R8有8个类别，分为2189个测试文件和5485个训练文件。R52有52个类别，分为6532个训练文件和2568个测试文件。</li>
<li><strong>Sogou</strong>：搜狗结合了两个数据集，包括SogouCA和SogouCS新闻集。每个文本的标签是URL中的域名。</li>
</ul>
<h3 id="3-1-3-Topic-Labeling-TL"><a href="#3-1-3-Topic-Labeling-TL" class="headerlink" title="3.1.3 Topic Labeling (TL)"></a>3.1.3 Topic Labeling (TL)</h3><p>主题分析试图通过定义复杂的语篇主题来获得语篇的意义。主题标注是主题分析技术的重要组成部分之一，旨在为每个文档分配一个或多个主题，以简化主题分析。主题标签数据集包括DBPedia、Ohsumed、Yahoo answers (Yahoo)、EUR-lex、Amazon670K、Bing、Fudan和PubMed。</p>
<ul>
<li><strong>DBpedia</strong>：DBpedia是使用维基百科最常用的信息框生成的大型多语言知识库。它每月发布DBpedia，在每个版本中添加或删除类和属性。DBpedia最流行的版本有14个类，分为560,000个训练数据和70,000个测试数据。</li>
<li><strong>Ohsumed</strong>：Ohsumed属于MEDLINE数据库。它包括7400篇文本，有23种心血管疾病类别。所有的文本都是医学文摘，并被标记为一个或多个类。</li>
<li><strong>YahooA</strong>：YahooA是一个带有10个类的主题标记任务。它包含14万训练数据和5000个测试数据。所有文本都包含三个元素，分别是题目、问题上下文和最佳答案。</li>
</ul>
<h3 id="3-1-4-Question-Answering-QA"><a href="#3-1-4-Question-Answering-QA" class="headerlink" title="3.1.4 Question Answering (QA)"></a>3.1.4 Question Answering (QA)</h3><p>QA任务可以分为两种类型:抽取式QA和生成式QA。抽取式QA为每个问题提供多个候选答案，以选择正确答案。因此，该文本分类模型可用于抽取性QA任务。本文所讨论的质量保证都是抽取性质量保证。问答系统可以应用文本分类模型识别正确答案，并将其他答案作为候选答案。问题回答数据集包括斯坦福问题回答数据集(Stanford Question Answering Dataset，SQuAD)、TREC-QA、WikiQA 、Subj、CR、MS MARCO和Quora。</p>
<ul>
<li><strong>SQuAD</strong>：SQuAD是一组从维基百科文章中获取的问答对。该数据集分为两类。SQuAD1.1包含536对107785问答题。SQuAD2.0将SQuAD1.1中的10万个问题，与复杂工作面临的5万个无法回答的问题，以可回答的问题类似的形式结合在一起。</li>
<li><strong>TREC-QA</strong>：包括5,452份训练文本和500份测试文本。它有两个版本。TREC-6包含6个类别，TREC-50包含50个类别。</li>
<li><strong>WikiQA</strong>：WikiQA数据集包含没有正确答案的问题，需要对答案进行评估。</li>
<li><strong>MS MARCO</strong>：MS MARCO包含问题和答案。这些问题和部分答案都是由必应搜索引擎从实际的网络文本中选取的。其他部分则是生成的。它被用于开发由微软发布的生成式QA系统。</li>
</ul>
<h3 id="3-1-5-Natural-Language-Inference-NLI"><a href="#3-1-5-Natural-Language-Inference-NLI" class="headerlink" title="3.1.5 Natural Language Inference (NLI)"></a>3.1.5 Natural Language Inference (NLI)</h3><p><strong>NLI用于预测一个文本的意思是否可以从另一个文本推导出来。释义是NLI的一种广义形，它通过测量句子对的语义相似度来决定一个句子是否是另一个句子的解释。</strong>NLI数据集包括斯坦福自然语言推理(Stanford Natural Language Inference，SNLI)、多体例自然语言推理(Multi-Genre Natural Language Inference，MNLI)、涉及构成知识的句子(Sentences Involving Compositional Knowledge，SICK)、微软研究转述(Microsoft Research Paraphrase ，MSRP)、语义文本相似性(Semantic Textual Similarity，STS)、文本蕴涵识别(Recognising<br>Textual Entailment，RTE)、科学分析(SciTail)等。</p>
<ul>
<li><strong>SNLI</strong>：SNLI一般应用于NLI任务。它包含570152个人工注释的句子对，包括训练集、验证集和测试集，用中性、蕴涵和矛盾三个类别进行注释。</li>
<li><strong>MNLI</strong>：MNLI是SNLI的扩展，涵盖了更广泛的书面语和口语文本体例。它包含43.3万个句子对，并用文本蕴涵标签加以注释。</li>
<li><strong>SICK</strong>：SICK包含了近10000对英语句子。它由中性的、隐含的和矛盾的标签组成。</li>
<li><strong>MSRP</strong>：MSRP由句子对组成，通常用于文本相似任务。每对注释都有一个二进制标签，以区分它们是否是释义，它分别包括1,725个训练集和4,076个测试集。</li>
</ul>
<h3 id="3-1-6-Multi-Label-ML-datasets"><a href="#3-1-6-Multi-Label-ML-datasets" class="headerlink" title="3.1.6 Multi-Label (ML) datasets"></a>3.1.6 Multi-Label (ML) datasets</h3><p>在多标签分类中，一个实例有多个标签，每个标签只能取多个类中的一个。基于多标签文本分类的数据集非常多。它包括路透社(Reuters)、路透社文集I卷(Reuters Corpus Volume I，RCV1)[255]、RCV1- 2k[255]、Arxiv学术论文数据集(Arxiv Academic Paper Dataset，AAPD)[117]、专利(Patent)、科学网络(Web of Science，WOS-11967)、AmazonCat-13K、blurbgenrecall(BGC)等。</p>
<ul>
<li><strong>Reuters</strong>：Reuters是路透社财经新闻服务中广泛使用的文本分类数据集。它有90个类别，7769个训练文本，3019个测试文本，包含多标签和单标签。还有一些Reuters的数据子集，如R8、BR52、RCV1和RCV1-v2。</li>
<li><strong>RCV1 and RCV1-2K</strong>：RCV1收集自1996年至1997年的路透社新闻报道，其中人类标记了103个类别。它分别包含23,149个训练文本和784,446个测试文本。RCV1- 2k数据集具有与RCV1相同的特征。然而，RCV1-2K的标签集已经扩展了一些新的标签。它包含2456个标签。</li>
<li><strong>AAPD</strong>：AAPD是计算机科学领域用于多标签文本分类的大型数据集。全文共55,840篇，包括摘要及相关课题，共54个标签。目的是根据摘要预测每篇论文的相应主题。</li>
<li><strong>Patent Dataset</strong>：专利数据集来自USPTO，这是一个包含标题和摘要等文本细节的美国专利系统。它包含了10万项美国在现实世界中授予的专利，有多个分级类别。</li>
<li><strong>WOS-11967</strong>：WOS-11967是从Web of Science上抓取的，由发表的论文摘要组成，每个例子都有两个标签。它更浅，但更宽，总类更少。</li>
</ul>
<h3 id="3-1-7-其他数据集"><a href="#3-1-7-其他数据集" class="headerlink" title="3.1.7 其他数据集"></a>3.1.7 其他数据集</h3><p>还有一些用于其他任务的数据集，例如SemEval-2010 Task 8、ACE 2003-2004、TACRED、NYT-10、FewRel 、Dialog State Tracking、Challenge 4 (DSTC 4)、ICSI Meeting Recorder Dialog Act (MRDA) 、Switchboard Dialog Act (SwDA)等</p>
<h2 id="3-2-评价指标"><a href="#3-2-评价指标" class="headerlink" title="3.2 评价指标"></a>3.2 评价指标</h2><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/metrics_notations.png" alt="metrics notations"></p>
<h3 id="3-2-1-Single-label-metrics"><a href="#3-2-1-Single-label-metrics" class="headerlink" title="3.2.1 Single-label metrics"></a>3.2.1 Single-label metrics</h3><p>单标签文本分类将文本划分为最可能应用于NLP任务（如QA、SA和对话系统）的类别之一。对于单标签文本分类，一个文本只属于一个目录，可以不考虑标签之间的关系。</p>
<ul>
<li><strong>Accuracy and ErrorRate</strong>：</li>
</ul>
<p>$$<br>Accuracy&#x3D;\frac{TP+TN}{N},<br>$$</p>
<p>$$<br>ErrorRate&#x3D;1-Accuracy&#x3D;\frac{FP+FN}{N}​<br>$$</p>
<ul>
<li><strong>Precision, Recall and F1：</strong></li>
</ul>
<p>$$<br>Precision&#x3D;\frac{TP}{TP+FP},\quad Recall&#x3D;\frac{TP}{TP+FN},<br>$$</p>
<p>$$<br>F1&#x3D;\frac{2 \times Precision \times Recall }{Precision+Recall}<br>$$</p>
<p>当Precision、Recall和F1值达到1时，将得到预期的结果。反之，当值为0时，则得到最差的结果。对于多类分类问题，可以分别计算每个类的精度和召回值，进而分析个体和整体的性能。</p>
<ul>
<li>**Exact Match (EM)**：EM是QA任务的一个度量标准，它精确地衡量与所有基本事实答案相匹配的预测。它是在SQuAD数据集上使用的主要度量。</li>
<li>**Mean Reciprocal Rank (MRR)**：MRR通常用于评估QA和信息检索(IR)任务中排序算法的性能。</li>
</ul>
<p>$$<br>MPR&#x3D;\frac{1}{Q}\sum_{i&#x3D;1}^Q \frac{1}{rank(i)}<br>$$</p>
<p>其中，$rank(i)$表示正确答案在预测答案的第$i$个位置。</p>
<ul>
<li><strong>Hamming-Loss (HL)：</strong>HL评估错误分类的实例-标签对的得分，其中一个相关的标签被省略或一个不相关的标签被预测。</li>
</ul>
<h3 id="3-2-2-Multi-label-metrics"><a href="#3-2-2-Multi-label-metrics" class="headerlink" title="3.2.2 Multi-label metrics"></a>3.2.2 Multi-label metrics</h3><p>与单标签文本分类相比，多标签文本分类将文本划分为多个类别标签，并且类别标签的数量是可变的。上述这些指标是为单标签文本分类而设计的，不适合多标签任务。因此，有一些为多标签文本分类设计的指标。</p>
<ul>
<li><strong>$Micro-F1$：</strong>一种考虑所有标签的整体准确性和召回率的测量方法。</li>
</ul>
<p>$$<br>Micro-F1&#x3D;\frac{2 \times P_t \times R_t}{P+R}, \<br>P&#x3D;\frac{\sum_{t \in S}TP_t}{\sum_{t \in S}{TP_t+FP_t}},\quad R&#x3D;\frac{\sum_){t \in S}TP_t}{\sum_{t \in S}{TP_t|FN_t}}<br>$$</p>
<ul>
<li><strong>$Macro-F1$：</strong>计算所有标签的平均F1。</li>
</ul>
<p>$$<br>Macro-F1&#x3D;\frac{1}{S}\sum_{t \in S}\frac{2 \times P_t \times R_t}{P_t+R_t}, \<br>P_t&#x3D;\frac{TP_t}{TP_t+FP_t}, \quad R_t&#x3D;\frac{TP_t}{TP_t+FN_t}<br>$$</p>
<ul>
<li><strong>Precision at Top K (P@K)：</strong>top K评估的精度。</li>
</ul>
<p>$$<br>P@K &#x3D; \frac{1}{K}\sum_{j&#x3D;0}^{min(L,K)-1}rel_{L_i}(P_t{j}), \<br>rel_L(p)&#x3D;<br>\left{<br>\begin{array}{}<br>1 \quad if \ p \in L &amp; \<br>0 \quad otherwise<br>\end{array}<br>\right.<br>$$</p>
<p>其中，$L$表示正确标签$L_t&#x3D;{l_0,l_1,l_2,…,l_{L-1}}$，$P_t$表示逐渐下降的概率得分$P_t&#x3D;[p_0,p_1,p_2,…p_{Q-1}]$。</p>
<ul>
<li><strong>Normalized Discounted Cummulated Gains (NDCG@K)：</strong></li>
</ul>
<p>$$<br>NDCG@K&#x3D;\frac{1}{IDCG(L_i,K)}\sum_{j&#x3D;1}^{K}\frac{rel_{L_i}(P_t(j))}{ln(j+1)},\<br>IDCG(L_i,K)&#x3D;\sum_{j&#x3D;1}^{n}\frac{rel_{L_i}(P_t(j))}{log_2(j+1)}, \<br>n&#x3D;min(max(|P_i|,|L_i|), K)<br>$$</p>
<ul>
<li><strong>总结</strong></li>
</ul>
<p>$Micro-F1$考虑了每个类别数目的不一致，$Macro-F1$没有考虑数目的影响，而是把每个类别都同等对待。当类别数据很大时可以考虑使用$P@K,NDCG@K$。</p>
<h1 id="4-模型比较"><a href="#4-模型比较" class="headerlink" title="4 模型比较"></a>4 模型比较</h1><p><img src="/medias/loading.gif" data-original="https://img.fdchen.host/TextClassificationCompare.png" alt="文本分类模型的性能比较"></p>
<p>总的来说，深度学习模型要比传统模型的效果好，并且基于与训练模型的方法比一般的深度学习模型性能更优。</p>
<h1 id="5-未来挑战"><a href="#5-未来挑战" class="headerlink" title="5 未来挑战"></a>5 未来挑战</h1><p>虽然一些新的文本分类模型反复刷高了大部分分类任务的准确率指标，但无法表明模型是否像人类一样从语义层面“理解”文本。而且，随着噪声样本的出现，小样本噪声可能导致决策置信度发生实质性变化，甚至导致决策逆转。</p>
<p>因此，模型的语义表示能力和鲁棒性需要在实践中得到验证。此外，预先训练的以词向量为代表的语义表示模型通常可以提高下游NLP任务的性能。现有的关于无上下文词向量的迁移策略的研究还比较初级。因此，我们从数据、模型和性能的角度得出结论，文本分类主要面临以下挑战。</p>
<h2 id="5-1-数据方面的挑战"><a href="#5-1-数据方面的挑战" class="headerlink" title="5.1 数据方面的挑战"></a>5.1 数据方面的挑战</h2><blockquote>
<p>数据方面的挑战是一个具有前景的方向，如下四个方向均可做参考</p>
</blockquote>
<p>主要研究的文本数据包括多篇章、短文本、跨语言、多标签、少样本文本。针对这些数据的特点，现有的技术挑战如下：</p>
<h3 id="5-1-1-零样本或少样本学习"><a href="#5-1-1-零样本或少样本学习" class="headerlink" title="5.1.1 零样本或少样本学习"></a>5.1.1 零样本或少样本学习</h3><p>目前的模型过于依赖大量的标记数据。这些模型的性能受到零样本和少样本学习的显著影响。因此，一些作品致力于解决这些问题。<strong>其主要思想是通过学习各种语义知识来推断特征，例如类之间的学习关系和结合类描述。此外，潜在特征生成、元学习和动态记忆机制也是有效的方法。</strong>但是，由于可见类数据少，可见类和可见类之间数据分布不同，要达到与人类相当的学习能力还有很长的路要走。</p>
<h3 id="5-1-2-外部知识"><a href="#5-1-2-外部知识" class="headerlink" title="5.1.2 外部知识"></a>5.1.2 外部知识</h3><p>众所周知，DNN中输入的有益信息越多，其性能就越好。例如，一个包含常识知识库的问答系统可以回答关于现实世界的问题，并帮助解决信息不完全的问题。因此，<strong>增加外部知识(知识库或知识图)是提高模型性能的有效途径。现有知识包括概念信息、常识知识、知识库信息、常识图等，增强了文本的语义表示能力。</strong>然而，由于投入规模的限制，如何以及为不同的任务增加什么仍然是一个挑战。</p>
<h3 id="5-1-3-具有许多术语的特殊领域"><a href="#5-1-3-具有许多术语的特殊领域" class="headerlink" title="5.1.3 具有许多术语的特殊领域"></a>5.1.3 具有许多术语的特殊领域</h3><p>现有的模型大多是监督模型，过度依赖大量的标记数据。当样本量过小，或出现零样本时，模型的性能会受到显著影响。新数据集注释需要大量时间。因此，无监督学习和半监督学习在文本分类中具有很大的潜力。此外，<strong>特定领域的文本，如金融和医学文本，包含许多特定的单词或领域专家可理解的俚语、缩写等，这使得现有的预先训练的词向量具有挑战性。</strong></p>
<h3 id="5-1-4-多标签的文本分类任务"><a href="#5-1-4-多标签的文本分类任务" class="headerlink" title="5.1.4 多标签的文本分类任务"></a>5.1.4 多标签的文本分类任务</h3><p><strong>多标签文本分类需要充分考虑标签之间的语义关系，模型的嵌入和编码是一个有损压缩的过程。因此，如何在训练过程中减少分层语义的损失，保留丰富而复杂的文档语义信息，仍然是一个需要解决的问题。</strong></p>
<h2 id="5-2-模型方面的挑战"><a href="#5-2-模型方面的挑战" class="headerlink" title="5.2 模型方面的挑战"></a>5.2 模型方面的挑战</h2><p>大多数传统和深度学习模型的现有结构都被尝试用于文本分类，包括集成方法。BERT学习了一种语言表示，可用于对许多NLP任务进行微调。主要的方法是增加数据，提高计算能力，设计训练程序以获得更好的结果[301-303]。如何在数据和计算资源与预测性能之间进行权衡是值得研究的。</p>
<h3 id="5-2-1-文本表征"><a href="#5-2-1-文本表征" class="headerlink" title="5.2.1 文本表征"></a>5.2.1 文本表征</h3><p><strong>在文本预处理阶段，基于向量空间模型的文本表示方法简单有效。但是，该方法会丢失文本的语义信息，因此应用性能有限。本文提出的基于语义的文本表示方法过于耗时。因此，基于语义的高效文本表示方法仍需进一步研究。</strong>在基于深度学习的文本分类文本表示中，词嵌入是主要的概念，而表示单元在不同的语言中有不同的描述。然后，通过模型学习映射规则，将单词表示为向量的形式。因此，如何设计自适应的数据表示方法更有利于深度学习与具体分类任务的结合。</p>
<h3 id="5-2-2-模型集成"><a href="#5-2-2-模型集成" class="headerlink" title="5.2.2 模型集成"></a>5.2.2 模型集成</h3><p>RNN需要逐级递归来获取全局信息。CNN可以获取局部信息，通过多层叠加增加感知场，获取更全面的上下文信息。注意机制学习句子中单词之间的全局依赖性。Transformer模型依赖于注意机制，以建立输入和输出之间全局依赖关系的深度。因此，<strong>设计一个集成模型尝试利用这些模型是值得的。</strong></p>
<h3 id="5-2-3-模型效率"><a href="#5-2-3-模型效率" class="headerlink" title="5.2.3 模型效率"></a>5.2.3 模型效率</h3><p>虽然基于深度学习的文本分类模型是非常有效的，如CNN、RNN和GNN。然而，网络层的深度、正则化问题、网络学习率等技术上存在许多局限性。因此，<strong>优化算法和提高模型训练速度仍有更广阔的发展空间。</strong></p>
<h2 id="5-3-性能方面的挑战"><a href="#5-3-性能方面的挑战" class="headerlink" title="5.3 性能方面的挑战"></a>5.3 性能方面的挑战</h2><p>传统模型和深度模型在大多数文本分类任务中都能取得较好的性能，但其结果的抗干扰能力有待提高。如何实现对深层模型的解释也是一个技术挑战。</p>
<h3 id="5-3-1-模型的语义鲁棒性"><a href="#5-3-1-模型的语义鲁棒性" class="headerlink" title="5.3.1 模型的语义鲁棒性"></a>5.3.1 模型的语义鲁棒性</h3><p><strong>近年来，研究人员设计了许多模型来提高文本分类模型的准确性。然而，当数据集中存在一些对抗样本时，模型的性能显著下降。对抗训练是提高预训练模型鲁棒性的重要方法。</strong>例如，一种流行的方法是将攻击转化为防御，并使用对抗样本训练模型。因此，如何提高模型的鲁棒性是当前研究的热点和挑战。</p>
<h3 id="5-3-2-模型的可解释性"><a href="#5-3-2-模型的可解释性" class="headerlink" title="5.3.2 模型的可解释性"></a>5.3.2 模型的可解释性</h3><p>DNN在特征提取和语义挖掘方面具有独特的优势，实现了优秀的文本分类任务。只有更好地理解这些模型背后的理论，才能为各种应用程序准确地设计更好的模型。然而，深度学习是一个黑盒模型，训练过程难以再现，隐式语义和输出可解释性较差。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">fdChen</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://blog.fdchen.host/2022/nlp-wen-ben-fen-lei-zong-shu/">https://blog.fdchen.host/2022/nlp-wen-ben-fen-lei-zong-shu/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">fdChen</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                    <span class="chip bg-color">学习笔记</span>
                                </a>
                            
                                <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">
                                    <span class="chip bg-color">文本分类</span>
                                </a>
                            
                                <a href="/tags/%E7%BB%BC%E8%BF%B0/">
                                    <span class="chip bg-color">综述</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/valine/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'tcK03mL1mpaBq1nRNoEYy3e4-gzGzoHsz',
        appKey: 'h8N9NwAkLQWY7wCMIufRFw18',
        notify: 'true' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: '',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: '填写昵称与邮箱即可添加评论，支持通过Gravatar official graphics设置头像，欢迎交流讨论~',
        requiredFields: ['nick','mail'], //设置必填项
    });
</script>

<!--酷Q推送-->


    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2022/cheng-xu-yuan-git-yu-github-su-cheng-jiao-cheng/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/loading.gif" data-original="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/medias/featureimages/8.jpg" class="responsive-img" alt="Git 与 GitHub 速成教程">
                        
                        <span class="card-title">Git 与 GitHub 速成教程</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            1 文字版教程转载：《史上最详细的Git教程》
2 视频教程转载：《【教程】学会Git玩转Github【全】》
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-12-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%8C%BF%E5%9F%BA%E7%A1%80/" class="post-category">
                                    猿基础
                                </a>
                            
                            <a href="/categories/%E7%8C%BF%E5%9F%BA%E7%A1%80/Git-%E4%B8%8E-GitHub/" class="post-category">
                                    Git 与 GitHub
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%95%99%E7%A8%8B/">
                        <span class="chip bg-color">教程</span>
                    </a>
                    
                    <a href="/tags/Git/">
                        <span class="chip bg-color">Git</span>
                    </a>
                    
                    <a href="/tags/GitHub/">
                        <span class="chip bg-color">GitHub</span>
                    </a>
                    
                    <a href="/tags/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">
                        <span class="chip bg-color">常用命令</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/ai-shen-jing-wang-luo-xian-jin-ji-zhu/">
                    <div class="card-image">
                        
                        <img src="/medias/loading.gif" data-original="https://img.fdchen.host/R-C.jpg" class="responsive-img" alt="神经网络先进技术">
                        
                        <span class="card-title">神经网络先进技术</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            总结目前比较热门的先进神经网络技术，实时更新
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-10-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AI/" class="post-category">
                                    AI
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">学习笔记</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: fdChen的掉发收集箱<br />'
            + '文章作者: fdChen<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('1'),
            headingSelector: 'h1, h2, h3, h4, h5'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4, h5').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
    <script src="/js/prism/prism.js" async></script>

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2020-2023</span>
            
            <a href="/about" target="_blank">fdChen</a>
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">416.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2020";
                        var startMonth = "3";
                        var startDate = "7";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
                <span id="icp"><img src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/medias/icp.png"
                                    style="vertical-align: text-bottom;"/>
                <a href="/beian.miit.gov.cn" target="_blank">湘ICP备20016057号</a>
            </span>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/CCSemicircle" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:fangd.chen@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我: fangd.chen@gmail.com" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2914756796" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2914756796" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>





    <a href="https://www.zhihu.com/people/semi-circle-42/posts" class="tooltipped" target="_blank" data-tooltip="关注我的知乎" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>

    
    <div id="loading-box">
      <div class="loading-left-bg"></div>
      <div class="loading-right-bg"></div>
      <div class="spinner-box">
        <div class="configure-border-1">
          <div class="configure-core"></div>
        </div>
        <div class="configure-border-2">
          <div class="configure-core"></div>
        </div>
        <div class="loading-word">加载中...</div>
      </div>
    </div>
    <!-- 页面加载动画 -->
    <script>
      $(document).ready(function () {
        document.body.style.overflow = 'auto';
        document.getElementById('loading-box').classList.add("loaded")
      })
    </script>
  
  


    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/js/matery.js"></script>

    <!-- 引入Googlefonts -->
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200;300;400;500;600;700;900&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800&family=Noto+Serif+SC:wght@200;300;400;500;600;700;900&display=swap');
    </style>




    

    

        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/others/sakura-small.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

    
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?4a943d1ca56da567bf16a9c4e2d1368e";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    
    <script type="text/javascript" color="18, 150, 219"
        pointColor="18, 150, 219" opacity='0.7'
        zIndex="-1" count="99"
        src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/background/canvas-nest.js"></script>
    

    

    

    
    <script src="https://cdn.jsdelivr.net/gh/ccsemicircle/cdn/libs/instantpage/instantpage.js" type="module"></script>
    

<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var e=n.imageLazyLoadSetting.isSPA,i=n.imageLazyLoadSetting.preloadRatio||1,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight*i||document.documentElement.clientHeight*i)&&t.top<=(n.innerHeight+240||document.documentElement.clientHeight+240)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},t.src!==i&&(n.src=i)}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
